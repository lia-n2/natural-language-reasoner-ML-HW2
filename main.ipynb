{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import csv\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
      ],
      "metadata": {
        "id": "s4sCWOJw14LN"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
        "import numpy as np\n",
        "import torch\n",
        "import pandas as pd\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AdamW\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load pre-trained tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"albert-xxlarge-v2\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"albert-xxlarge-v2\", num_labels=2)\n",
        "model = torch.nn.DataParallel(model, output_device=6)\n",
        "model.to(device)\n",
        "\n",
        "\n",
        "for name, param in model.named_parameters():\n",
        "     if 'albert.embeddings' in name or 'embedding' in name:\n",
        "        param.requires_grad = False\n",
        "\n",
        "for name, param in model.named_parameters():\n",
        "     print(name, param.requires_grad)\n",
        "\n",
        "# Load the data\n",
        "train = pd.read_csv('pnli_train.csv', encoding='utf-8', header=None)\n",
        "dev = pd.read_csv('pnli_dev.csv', encoding='utf-8', header=None)\n",
        "\n",
        "train = np.array(train)\n",
        "dev = np.array(dev)\n",
        "precondition_train, statement_train, label_train = train[:, 0], train[:, 1], train[:, 2]\n",
        "precondition_dev, statement_dev, label_dev = dev[:,0], dev[:,1], dev[:,2]\n",
        "\n",
        "# Combine precondition and statement for training and dev sets\n",
        "X_train = []\n",
        "for pre, sta in zip(precondition_train, statement_train):\n",
        "    X_train.append(f\"{pre} {sta}\")\n",
        "\n",
        "X_train = [str(X) for X in X_train]\n",
        "y_train = [int(y) for y in label_train]\n",
        "\n",
        "\n",
        "X_dev = []\n",
        "for pred, state in zip(precondition_dev, statement_dev):\n",
        "    X_dev.append(f\"{pred} {state}\")\n",
        "\n",
        "X_dev = [str (X) for X in X_dev]\n",
        "y_dev = [int(y) for y in label_dev]\n",
        "\n",
        "\n",
        "\n",
        "# Tokenize input sentences\n",
        "X_train_encoded = tokenizer(X_train, padding=True, truncation=True, return_tensors=\"pt\", max_length=128)\n",
        "X_dev_encoded = tokenizer(X_dev, padding=True, truncation=True, return_tensors=\"pt\", max_length=128)\n",
        "\n",
        "\n",
        "# Create DataLoader for train and dev sets\n",
        "train_dataset = TensorDataset(X_train_encoded[\"input_ids\"], X_train_encoded[\"attention_mask\"], torch.tensor(y_train))\n",
        "dev_dataset = TensorDataset(X_dev_encoded[\"input_ids\"], X_dev_encoded[\"attention_mask\"], torch.tensor(y_dev))\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=8, drop_last=True)\n",
        "dev_loader = DataLoader(dev_dataset, batch_size=16, num_workers=8, shuffle=True, drop_last=True)\n",
        "\n",
        "# Fine-tune the model\n",
        "optimizer = torch.optim.RAdam(model.parameters(), lr=2e-5, weight_decay=0.01)\n",
        "\n",
        "# Set up warm-up steps\n",
        "total_steps = len(train_loader) * 5  # 10 epochs\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=total_steps // 10, num_training_steps=total_steps)\n",
        "\n",
        "base_dev_loss = 1000\n",
        "for epoch in range(10):\n",
        "    model.train()\n",
        "    for batch in train_loader:\n",
        "        input_ids, attention_mask, labels = batch\n",
        "        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        loss = loss.mean()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()  # Update learning rate\n",
        "\n",
        "    # Evaluate on dev set\n",
        "    model.eval()\n",
        "    dev_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in dev_loader:\n",
        "            input_ids, attention_mask, labels = batch\n",
        "            input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
        "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            dev_loss += outputs.loss.mean().item()\n",
        "            predicted_labels = torch.argmax(outputs.logits, dim=1)\n",
        "            correct += (predicted_labels == labels).sum().item()\n",
        "            total += len(labels)\n",
        "\n",
        "    accuracy = correct / total\n",
        "    print(f\"Epoch {epoch+1}: Dev Loss = {dev_loss/len(dev_loader):.4f}, Accuracy = {accuracy:.4f}\")\n",
        "\n",
        "    if base_dev_loss > dev_loss:\n",
        "        print(\"Replace best model\")\n",
        "        base_dev_loss=dev_loss\n",
        "        best_model = copy.deepcopy(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SMElgpn21id0",
        "outputId": "fdac4c43-b66f-40df-aff8-f31b5d780169"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at albert-xxlarge-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "module.albert.embeddings.word_embeddings.weight False\n",
            "module.albert.embeddings.position_embeddings.weight False\n",
            "module.albert.embeddings.token_type_embeddings.weight False\n",
            "module.albert.embeddings.LayerNorm.weight False\n",
            "module.albert.embeddings.LayerNorm.bias False\n",
            "module.albert.encoder.embedding_hidden_mapping_in.weight False\n",
            "module.albert.encoder.embedding_hidden_mapping_in.bias False\n",
            "module.albert.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm.weight True\n",
            "module.albert.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm.bias True\n",
            "module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query.weight True\n",
            "module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query.bias True\n",
            "module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key.weight True\n",
            "module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key.bias True\n",
            "module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value.weight True\n",
            "module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value.bias True\n",
            "module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.dense.weight True\n",
            "module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.dense.bias True\n",
            "module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm.weight True\n",
            "module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm.bias True\n",
            "module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn.weight True\n",
            "module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn.bias True\n",
            "module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output.weight True\n",
            "module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output.bias True\n",
            "module.albert.pooler.weight True\n",
            "module.albert.pooler.bias True\n",
            "module.classifier.weight True\n",
            "module.classifier.bias True\n",
            "Epoch 1: Dev Loss = 0.3142, Accuracy = 0.8923\n",
            "Replace best model\n",
            "Epoch 2: Dev Loss = 0.2438, Accuracy = 0.9087\n",
            "Replace best model\n",
            "Epoch 3: Dev Loss = 0.2429, Accuracy = 0.9173\n",
            "Replace best model\n",
            "Epoch 4: Dev Loss = 0.2787, Accuracy = 0.9000\n",
            "Epoch 5: Dev Loss = 0.2924, Accuracy = 0.9135\n",
            "Epoch 6: Dev Loss = 0.2821, Accuracy = 0.9163\n",
            "Epoch 7: Dev Loss = 0.2893, Accuracy = 0.9144\n",
            "Epoch 8: Dev Loss = 0.2929, Accuracy = 0.9135\n",
            "Epoch 9: Dev Loss = 0.2922, Accuracy = 0.9135\n",
            "Epoch 10: Dev Loss = 0.2821, Accuracy = 0.9163\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8OYSfyqT1ig9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Loading best model')\n",
        "model = best_model\n",
        "\n",
        "model.eval()\n",
        "dev_loss = 0\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for batch in dev_loader:\n",
        "        input_ids, attention_mask, labels = batch\n",
        "        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        dev_loss += outputs.loss.item()\n",
        "        predicted_labels = torch.argmax(outputs.logits, dim=1)\n",
        "        correct += (predicted_labels == labels).sum().item()\n",
        "        total += len(labels)\n",
        "\n",
        "accuracy = correct / total\n",
        "print(f\"Final: Dev Loss = {dev_loss/len(dev_loader):.4f}, Accuracy = {accuracy:.4f}\")\n",
        "\n",
        "\n",
        "# Make predictions on the unlabeled test set\n",
        "test = pd.read_csv('pnli_test_unlabeled.csv', encoding='utf-8', header=None)\n",
        "test = np.array(test)\n",
        "X_test = []\n",
        "precondition_test, statement_test = test[:,0], test[:,1]\n",
        "for pred, state in zip(precondition_test, statement_test):\n",
        "    X_test.append(f\"{pred} {state}\")\n",
        "\n",
        "X_test = [str (X) for X in X_test]\n",
        "X_test_encoded = tokenizer(X_test, padding=True, truncation=True, return_tensors=\"pt\", max_length=128)\n",
        "test_dataset = TensorDataset(X_test_encoded[\"input_ids\"], X_test_encoded[\"attention_mask\"])\n",
        "test_loader = DataLoader(test_dataset, batch_size=50, num_workers=8)\n",
        "\n",
        "# Predict on test set\n",
        "results = []\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        input_ids, attention_mask = batch\n",
        "        input_ids, attention_mask = input_ids.to(device), attention_mask.to(device)\n",
        "        outputs = model(input_ids, attention_mask=attention_mask)\n",
        "        predicted_labels = torch.argmax(outputs.logits, dim=1)\n",
        "        results.extend(predicted_labels.cpu().numpy())\n",
        "\n",
        "results = [int(x) for x in results]\n",
        "with open('upload_predictions_albert.txt', 'w', encoding = 'utf-8') as fp:\n",
        "    for x in results:\n",
        "        fp.write(str(x) + '\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tORPb-zb1oIk",
        "outputId": "1aa467b8-466e-482b-e536-179fc7340ddd"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading best model\n",
            "Final: Dev Loss = 0.2423, Accuracy = 0.9183\n"
          ]
        }
      ]
    }
  ]
}